# -*- coding: utf-8 -*-
"""Stroke_DataSet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCo8ZAGrjUHuLBHfrkR7yFlsVqQQfQj9
"""

#Key libraries being imported
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn as sc
import random

from ipywidgets import interact_manual
import plotly.express as px

from scipy.stats import zscore

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

from sklearn.preprocessing import OrdinalEncoder
ord_enc = OrdinalEncoder()

from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
from sklearn import preprocessing

#Saving the data as a datafram from Pandas library
df = pd.read_csv('/content/drive/MyDrive/stroke_data.csv')
#tr = df.sample(frac=1)
#tr.head()

df.head()

"""I can use training and testing data by randomly shuffling the entire data set and using x rows for traning and 40,000 - x rows for testing.

https://datagy.io/pandas-shuffle-dataframe/
"""

print(df.shape)
#print(tr.shape)

#Dropping any rows which have null values for any parameter
df = df.dropna()
df.head()

print(df.shape)

tf = pd.read_csv('/content/drive/MyDrive/stroke_data.csv')
tf.drop('stroke',inplace=True, axis=1)

df.describe()

#Example of a box plot for one quantitative parameter
df["bmi"].plot(kind = "box")
plt.ylabel("BMI Values")
plt.xlabel("BMI")
plt.title("BMI Box Plot")

#Removing outliers of BMI from the data set to ensure proper distribution of data
Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR

# Create arrays of Boolean values indicating the outlier rows
upper_array = np.where(df['bmi']>=upper)[0]
lower_array = np.where(df['bmi']<=lower)[0]

# Removing the outliers
df.drop(index=upper_array, inplace=True)
df.drop(index=lower_array, inplace=True)

#Removing outliers of avg glucose levels from the data set to ensure proper distribution of data
Q1 = df['avg_glucose_level'].quantile(0.25)
Q3 = df['avg_glucose_level'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR

# Create arrays of Boolean values indicating the outlier rows
upper_array = np.where(df['avg_glucose_level']>=upper)[0]
lower_array = np.where(df['avg_glucose_level']<=lower)[0]

# Removing the outliers
df.drop(index=upper_array, inplace=True)
df.drop(index=lower_array, inplace=True)

#Removing outliers of age from the data set to ensure proper distribution of data
Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR

# Create arrays of Boolean values indicating the outlier rows
upper_array = np.where(df['age']>=upper)[0]
lower_array = np.where(df['age']<=lower)[0]

# Removing the outliers
df.drop(index=upper_array, inplace=True)
df.drop(index=lower_array, inplace=True)

bmi_values = df['bmi'].tolist()

# Create a horizontal box and whisker plot
plt.boxplot(bmi_values, vert=False)

# Customize labels and title
plt.xlabel('BMI Values')
plt.title('Box and Whisker Plot for BMI')

# Show the plot
plt.show()

Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outlier removal
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df_cleaned = df[(df['bmi'] >= lower_bound) & (df['bmi'] <= upper_bound)]

plt.boxplot(df_cleaned['bmi'], vert=False)
plt.xlabel('BMI Values')
plt.title('Box and Whisker Plot for BMI (After Removing Outliers)')
plt.show()

avg_gluc = df['avg_glucose_level'].tolist()

# Create a horizontal box and whisker plot
plt.boxplot(avg_gluc, vert=False)

# Customize labels and title
plt.xlabel('Average Glucose Level Values')
plt.title('Box and Whisker Plot for Average Glucose Level')

# Show the plot
plt.show()

Q1 = df['avg_glucose_level'].quantile(0.25)
Q3 = df['avg_glucose_level'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outlier removal
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df_cleaned_gluc = df[(df['avg_glucose_level'] >= lower_bound) & (df['avg_glucose_level'] <= upper_bound)]


plt.boxplot(df_cleaned_gluc['avg_glucose_level'], vert=False)
plt.xlabel('Average Glucose Values')
plt.title('Box and Whisker Plot for Average Glucose Values(After Removing Outliers)')
plt.show()

age = df['age'].tolist()

# Create a horizontal box and whisker plot
plt.boxplot(age, vert=False)

# Customize labels and title
plt.xlabel('Age')
plt.title('Horizontal Box and Whisker Plot for Ages')

# Show the plot
plt.show()

Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outlier removal
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df_cleaned_gluc = df[(df['age'] >= lower_bound) & (df['age'] <= upper_bound)]


plt.boxplot(df_cleaned_gluc['age'], vert=False)
plt.xlabel('Age')
plt.title('Box and Whisker Plot for Ages(After Removing Outliers)')
plt.show()

print(df.shape)

"""922 data values were outliers (this amounts to 0.023% of the entire data set)"""

df.sample(3)

df.query("avg_glucose_level < 100")

df.loc[25004,"age"]

df.iloc[25004,1]

#Scatter plot of sex vs bmi
df.plot(x = 'bmi',y = 'sex',kind = 'scatter')
plt.show()

#Different sets of scatter plots
df.plot(x = 'bmi',y = 'stroke',kind = 'scatter')
df.plot(x = 'avg_glucose_level', y = 'stroke', kind = 'scatter')
df.plot(x = 'age', y = 'stroke', kind = 'scatter')
df.plot(x = 'sex', y = 'stroke', kind = 'scatter')
df.plot(x = 'hypertension', y = 'stroke', kind = 'scatter')
df.plot(x = 'ever_married', y = 'stroke', kind = 'scatter')
df.plot(x = 'heart_disease', y = 'stroke', kind = 'scatter')
df.plot(x = 'smoking_status', y = 'stroke', kind = 'scatter')
df.plot(x = 'Residence_type', y = 'stroke', kind = 'scatter')
df.plot(x = 'work_type', y = 'stroke', kind = 'scatter')

plt.show()

"""Age vs BMI -- No relationship
Avg_Glucose_Level vs BMI -- No relationship
Avg_glucose_level vs Age -- No relationship
"""

#Different set of histograms
df.hist(column = "bmi")
df.hist(column = "avg_glucose_level")
df.hist(column = "age")
df.hist(column = "sex")
df.hist(column = "hypertension")
df.hist(column = "ever_married")
df.hist(column = "heart_disease")
df.hist(column = "smoking_status")
df.hist(column = "Residence_type")
df.hist(column = "work_type")

"""BMI:
Center close to 30; right-skewed distribution

Average Glucose Level:
Center close to 110-120; bimodal distribution

Age:
Center close to 50; Normal distribution
"""

stroke_counts = df['stroke'].value_counts()

# Create a bar plot
plt.bar(stroke_counts.index, stroke_counts.values)

# Add labels and title
plt.xlabel('Stroke (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.title('Count of People with and without Stroke')

# Show the plot
plt.show()

# Assuming you have a DataFrame named 'df' with a column 'stroke' containing 0s and 1s
# For example:
# df = pd.read_csv("your_dataset.csv")

# Use value_counts() to count the occurrences of each unique value in the 'stroke' column
stroke_counts = df['stroke'].value_counts()

# Define colors for the bars (e.g., blue for 0, green for 1)
colors = ['blue', 'green']

# Create a bar plot with specified colors
plt.bar(stroke_counts.index, stroke_counts.values, color=colors)

# Set the x-axis tick labels explicitly
plt.xticks([0, 1], ['0 (No Stroke)', '1 (Stroke)'])

# Add labels and title
plt.xlabel('Stroke')
plt.ylabel('Count')
plt.title('Count of People with and without Stroke')

# Show the plot
plt.show()

# Use value_counts() to count the occurrences of each unique value in the 'stroke' column
stroke_counts = df['stroke'].value_counts()

# Display the counts
print(stroke_counts)

df.corr()

features_num = ['age', 'avg_glucose_level','bmi']

corr_spearman = df[features_num].corr(method='spearman')

fig = plt.figure(figsize = (6,5))
sns.heatmap(corr_spearman, annot=True, cmap="RdYlGn", vmin=-1, vmax=+1)
plt.title('Spearman Correlation')
plt.show()

#Showing a set of correlation between all the different parameter values
dataplot = sns.heatmap(df.corr(), cmap="YlGnBu", annot=True)
plt.show()


#REMOVE STROKE FROM TABLE

#Showing a set of correlation between all the different parameter values
plt.figure(figsize=(10, 8))
dataplot = sns.heatmap(tf.corr(), cmap="YlGnBu", annot=True)
plt.show()

"""High Correlation: > 0.8 or < -0.8
No high correlations in this data set

Low Correlation: close to 0.00

We can show distinct effect of the different variables.
"""

#Showing scatter plot of all the different sets of parameters with each other, keeping the marriage out of this set
#sns.pairplot(df.drop('ever_married', axis = 1))

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(),
                xticklabels=df.columns,
                yticklabels=df.columns)

sns.heatmap(tf.corr(),
            xticklabels=df.columns,
            yticklabels=df.columns)

#REMOVE STROKE FROM TABLE

"""Appears as if there is not data with high correlation to stroke."""

#Scaling the data to fit under the same set of axes so that there is no significant influence of one parameter over the other
standard = preprocessing.scale(df)
print(standard)

#Seperating the dataframe into two tables: one with only stroke and one with every parameter other than stroke
Z = df.drop(['stroke'],axis=1)
Z.head()

W = df.drop(df.columns[[0,1,2,3,4,5,6,7,8]],axis = 1)
W.tail()

#Scaling data for non-storke values
standardIV = preprocessing.scale(Z)
print(standardIV)

X = df.drop(['stroke'], axis=1)

y = df['stroke']

#Train-test split of 70-30%; the models produced were better with 70-30 split compared to 80-20
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""Model accuracy was better with 70-30 split rather than 80-20 split"""

#Difference in size between training and testing data
X_train.shape, X_test.shape

#Random Forest Classifier builds decision trees on different samples and takes their majority vote for classification and average in case of regression
from sklearn.ensemble import RandomForestClassifier

# instantiate the classifier
rfc = RandomForestClassifier(random_state=0)

# fit the model
rfc.fit(X_train, y_train)

# Predict the Test set results
y_pred = rfc.predict(X_test)

# Check accuracy score
from sklearn.metrics import accuracy_score

print('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

# instantiate the classifier with n_estimators = 100
rfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)

# fit the model to the training set
rfc_100.fit(X_train, y_train)

# Predict on the test set results
y_pred_100 = rfc_100.predict(X_test)


# Check accuracy score
print('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))

#View the feature scores
feature_scores = pd.Series(rfc.feature_importances_, index=X_train.columns).sort_values(ascending=False)
feature_scores

#None of the features have a very significant effect on the data so there shouldn't be much of an effect

# Creating a seaborn bar plot to investigate the feature scores
sns.barplot(x=feature_scores, y=feature_scores.index)

# Add labels to the graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')

# Add title to the graph
plt.title("Visualizing Important Features")

# Visualize the graph
plt.show()

"""10 vs 100 estimators has no effect on the data accuracy"""

# create the classifier with n_estimators = 100
clf = RandomForestClassifier(n_estimators=100, random_state=0)

# fit the model to the training set
clf.fit(X_train, y_train)

#View the feature scores
feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
feature_scores

#None of the features have a very significant effect on the data so there shouldn't be much of an effect

# Creating a seaborn bar plot to investigate the feature scores
sns.barplot(x=feature_scores, y=feature_scores.index)

# Add labels to the graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')

# Add title to the graph
plt.title("Visualizing Important Features")

# Visualize the graph
plt.show()

"""Including features even with score < 0.05 because it helps with model accuracy without resulting in overfitting"""

# Print the Confusion Matrix and slice it into four pieces
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)

#Classification report for the data set
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, roc_curve, classification_report

f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")


roc_auc = roc_auc_score(y_test, y_pred)
print(f"AUC: {roc_auc:.4f}")


precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")

#The data of the strokes are balanced (having and not having stroke)
df.hist(column = "stroke")

#Confusion Matrix
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#Creating Logistic Regression Model

#import the class
from sklearn.linear_model import LogisticRegression

# instantiate the model (using the default parameters)
logreg = LogisticRegression(random_state=16)

# fit the model with data
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

# Print the Confusion Matrix and slice it into four pieces
from sklearn.metrics import confusion_matrix
from sklearn import metrics

cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)
print(metrics.classification_report(y_test,y_pred))

#CNN Heatmap for Logistic Regression Model
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification

#Testing Accuracy Model for Logistic Regression
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

#F1 Score
import numpy as np
from sklearn.metrics import f1_score

#define array of actual classes
actual = np.repeat([1, 0], repeats=[5952, 6044])

#define array of predicted classes
pred = np.repeat([1, 0, 1, 0], repeats=[5929, 23, 0, 6044])

#calculate F1 score for LogReg model
f1_score(actual, pred)

#Area Under Curve (AUC) Valuation
from sklearn import metrics
#use model to predict probability that given y value is 1
y_pred_proba = logreg.predict_proba(X_test)[::,1]

#calculate AUC of model
auc = metrics.roc_auc_score(y_test, y_pred_proba)

#print AUC score
print(auc)

precision = precision_score(y_test, y_pred)
print(f"Precision Score: {precision:.4f}")

from sklearn.metrics import recall_score, classification_report
recall = recall_score(y_test, y_pred)
print(f"Recall Score: {recall:.4f}")

#KNN (K-nearest neighbors) Model
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)

# Predict on dataset which model has not seen before
print(knn.predict(X_test))

"""Cross Validation can be used to find the optimal number of neighbors."""

#To check for accuracy of KNN model
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over K values
for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    # Compute training and test data accuracy
    train_accuracy[i] = knn.score(X_train, y_train)
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')

plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.show()

#Comparing between 1 and 2 neighbors
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred))

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred))

"""KNN has better accuracy than the logistic regression model for accuracy score but F1 score for logreg is much higher"""

#Test Accuracy Score
accuracyKNN = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracyKNN)
print(knn.score(X_test,y_test))

#Confusion matrix for KNN Model
cmKNN = metrics.confusion_matrix(y_test, y_pred)
print(cmKNN)
print(metrics.classification_report(y_test,y_pred))

#CNN Heatmap for KNN Model
sns.heatmap(pd.DataFrame(cmKNN), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#F1 Score
import numpy as np
from sklearn.metrics import f1_score

#define array of actual classes
actualKNN = np.repeat([1, 0], repeats=[5952, 6044])

#define array of predicted classes
predKNN = np.repeat([1, 0, 1, 0], repeats=[4645, 1307, 130, 5914])

#calculate F1 score
f1_score(actualKNN, predKNN)

#AUC Valuation for KNN
from sklearn import metrics
#use model to predict probability that given y value is 1
y_pred_proba = knn.predict_proba(X_test)[::,1]

#calculate AUC of model
auc = metrics.roc_auc_score(y_test, y_pred_proba)

#print AUC score
print(auc)

precision = precision_score(y_test, y_pred)
print(f"Precision Score: {precision:.4f}")

recall = recall_score(y_test, y_pred)
print(f"Recall Score: {recall:.4f}")

#SVM Model
from seaborn import load_dataset, pairplot
from sklearn.svm import SVC

pairplot(df)
plt.show()

classifier = SVC(kernel='rbf', random_state = 1)
classifier.fit(X_train,y_train)

y_predSVM = classifier.predict(X_test)

svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)
svm.fit(X_train, y_train)

print('The accuracy of the svm classifier on training data is {:.2f} out of 1'.format(svm.score(X_train, y_train)))
print('The accuracy of the svm classifier on test data is {:.2f} out of 1'.format(svm.score(X_test, y_test)))

y_pred = svm.predict(X_test)
accuracySVM = accuracy_score(y_test, y_pred)
print(accuracySVM)

print(classification_report(y_test,y_pred))

#Confusion matrix for KNN Model
cmSVM = metrics.confusion_matrix(y_test, y_pred)
print(cmSVM)
print(metrics.classification_report(y_test,y_pred))

sns.heatmap(pd.DataFrame(cmSVM), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#AUC for SVM Classifier
print("area under curve (auc): ", metrics.roc_auc_score(y_test,y_pred))

precision = precision_score(y_test, y_pred)
print(f"Precision Score: {precision:.4f}")

recall = recall_score(y_test, y_pred)
print(f"Recall Score: {recall:.4f}")

f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")

knn = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')
knn.fit(X_train, y_train)

print('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(X_train, y_train)))
print('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(X_test, y_test)))

import xgboost as xgb

xgb_clf = xgb.XGBClassifier()
xgb_clf = xgb_clf.fit(X_train, y_train)

print('The accuracy of the xgb classifier is {:.2f} out of 1 on training data'.format(xgb_clf.score(X_train, y_train)))
print('The accuracy of the xgb classifier is {:.2f} out of 1 on test data'.format(xgb_clf.score(X_test, y_test)))

from xgboost import XGBClassifier


# declare parameters
params = {
            'objective':'binary:logistic',
            'max_depth': 4,
            'alpha': 10,
            'learning_rate': 1.0,
            'n_estimators':100
        }


# instantiate the classifier
xgb_clf = XGBClassifier(**params)


# fit the classifier to the training data
xgb_clf.fit(X_train, y_train)

print(xgb_clf)

#Accuracy score for XGB
y_pred = xgb_clf.predict(X_test)
print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

import xgboost as xgb


# define data_dmatrix
data_dmatrix = xgb.DMatrix(data=X,label=y)

from xgboost import cv

params = {"objective":"binary:logistic",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}

xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,
                    num_boost_round=50, early_stopping_rounds=10, metrics="auc", as_pandas=True, seed=123)

xgb_cv.head()

xgb.plot_importance(xgb_clf)
plt.figure(figsize = (16, 12))
plt.show()

#AUC for XGboost Classifier
print("area under curve (auc): ", metrics.roc_auc_score(y_test,y_pred))

f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")

"""Other models: Decision trees, random forest, bagging"""

precision = precision_score(y_test, y_pred)
print(f"Precision Score: {precision:.4f}")

recall = recall_score(y_test, y_pred)
print(f"Recall Score: {recall:.4f}")

#Confusion matrix for KNN Model
cmXGB = metrics.confusion_matrix(y_test, y_pred)
print(cmXGB)
print(metrics.classification_report(y_test,y_pred))

sns.heatmap(pd.DataFrame(cmXGB), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#Decision Tree Classifier
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test

feature_cols = ['avg_glucose_level', 'bmi', 'work_type', 'Residence_type','smoking_status','heart_disease','ever_married','hypertension','age','sex']

#Finding optimal decision tree depth via cross-validation

from sklearn.model_selection import cross_val_score, train_test_split

depths = range(1, 21)

cv_scores = []

for depth in depths:
    # Create a Decision Tree Classifier with the current depth
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # Perform k-fold cross-validation (e.g., 5-fold)
    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')

    # Append the mean accuracy score to the list of scores
    cv_scores.append(np.mean(scores))

plt.figure(figsize=(10, 6))
plt.plot(depths, cv_scores, marker='o', linestyle='-')
plt.title('Cross-Validation Scores vs. Tree Depth')
plt.xlabel('Tree Depth')
plt.ylabel('Mean Accuracy')
plt.grid(True)
plt.show()

optimal_depth = depths[np.argmax(cv_scores)]
print(f"Optimal Depth: {optimal_depth}")

clf = DecisionTreeClassifier(max_depth=optimal_depth, random_state=42)
clf.fit(X_train, y_train)
test_accuracy = clf.score(X_test, y_test)

print(f"Test Accuracy with Optimal Depth: {test_accuracy:.4f}")

for depth in (depths):
  clf = DecisionTreeClassifier(criterion="entropy", max_depth=depth)
  clf = clf.fit(X_train,y_train)
  y_pred = clf.predict(X_test)
  print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('stroke.png')
Image(graph.create_png())

from sklearn.metrics import roc_auc_score, roc_curve, auc

#AUC score
y_pred_prob = clf.predict_proba(X_test)[:, 1]
auc_roc = roc_auc_score(y_test, y_pred_prob)
print(f"AUC-ROC Score: {auc_roc:.4f}")

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

#F1 Score
from sklearn.metrics import f1_score, classification_report
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.4f}")

classification_rep = classification_report(y_test, y_pred)
print("Classification Report:\n", classification_rep)

precision = precision_score(y_test, y_pred)
print(f"Precision Score: {precision:.4f}")

recall = recall_score(y_test, y_pred)
print(f"Recall Score: {recall:.4f}")

#Confusion matrix for KNN Model
cmDecision = metrics.confusion_matrix(y_test, y_pred)
print(cmDecision)
print(metrics.classification_report(y_test,y_pred))

sns.heatmap(pd.DataFrame(cmDecision), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')